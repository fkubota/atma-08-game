{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "- nb008の改良　\n",
    "- nb009で見つけたハイパラを使用する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Const"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB = '010'\n",
    "N_SPLITS = 5\n",
    "SHOW_LOG = True\n",
    "# VERBOSE = 10\n",
    "VERBOSE = None\n",
    "EARLY_STOPPING_ROUNDS = 100\n",
    "\n",
    "\n",
    "PATH_TRAIN = './../data/official/train.csv'\n",
    "PATH_TEST = './../data/official/test.csv'\n",
    "PATH_SAMPLE_SUBMITTION = './../data/official/atmaCup8_sample-submission.csv'\n",
    "SAVE_DIR = f'../data/output_nb/nb{NB}/'\n",
    "\n",
    "feat_train_only = ['JP_Sales', 'Global_Sales', 'NA_Sales', 'Other_Sales', 'EU_Sales']\n",
    "feat_common = ['Name', 'Platform', 'Year_of_Release', 'Genre', 'Publisher',\n",
    "           'Critic_Score', 'Critic_Count', 'User_Score', 'User_Count', 'Developer',\n",
    "           'Rating']\n",
    "feat_string = ['Platform', 'Genre', 'Publisher', 'Developer', 'Rating']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import everything I need :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from lightgbm import LGBMRegressor \n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(y_true, y_pred):\n",
    "    return mean_squared_log_error(y_true, y_pred) ** .5\n",
    "\n",
    "def preprocess_User_Score(df):\n",
    "    '''\n",
    "    - tbdをnanにする\n",
    "    - stringをfloatにする\n",
    "    '''\n",
    "    mask = df.User_Score.values == 'tbd'\n",
    "    df.User_Score[mask] = np.nan\n",
    "    df.User_Score = df.User_Score.values.astype(float)\n",
    "    return df\n",
    "\n",
    "def string_encode(df, cols):\n",
    "    '''\n",
    "    - np.nanがあれば、'nan'に置き換える\n",
    "    - label encodingする\n",
    "    '''\n",
    "    df[cols] = df[cols].replace(np.nan, 'nan')\n",
    "    for col in cols:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "    return df\n",
    "\n",
    "def df_preprocessing(df, string_cols):\n",
    "    df = preprocess_User_Score(df)\n",
    "    df = string_encode(df, string_cols)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fold_lgbm(_X_trn, _y_trn, _X_val, _y_val, _X_te, model_config, show_log=True):\n",
    "    FEAT_STRING = ['Platform', 'Genre', 'Publisher', 'Developer', 'Rating']\n",
    "    \n",
    "    # train\n",
    "    model = LGBMRegressor(objective='regression', **model_config)\n",
    "    model.fit(_X_trn, _y_trn.values[:, 0],\n",
    "              categorical_feature=FEAT_STRING,\n",
    "              eval_set=[(_X_trn, _y_trn), (_X_val, _y_val)],\n",
    "              verbose=VERBOSE,\n",
    "              early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "             )\n",
    "    \n",
    "    # predict\n",
    "    y_trn_pred = model.predict(_X_trn)\n",
    "    y_val_pred = model.predict(_X_val)\n",
    "    _y_test_pred = model.predict(_X_te)\n",
    "    \n",
    "    # postprocessiing\n",
    "    y_trn_pred[y_trn_pred <= 1] = 1\n",
    "    y_val_pred[y_val_pred <= 1] = 1\n",
    "    \n",
    "    if show_log:\n",
    "        print(show_log)\n",
    "        print(f'score train: {metric(_y_trn, y_trn_pred):.5f}')\n",
    "        print(f'score valid: {metric(_y_val, y_val_pred):.5f}')\n",
    "        print('')\n",
    "    \n",
    "    return y_trn_pred, y_val_pred, _y_test_pred\n",
    "\n",
    "def run(X, y, X_te, splitter, use_col, model_config, show_log=True):\n",
    "    print(f'use_col: {use_col}\\n') if show_log else None\n",
    "\n",
    "    oof = np.zeros(len(X))\n",
    "    y_test_pred = np.zeros(len(X_te))\n",
    "    for fold_i, (idx_trn, idx_val) in enumerate(splitter.split(X)):\n",
    "        if show_log:\n",
    "            print(f'::Fold {fold_i+1}/{N_SPLITS} start at {time.ctime()}::')\n",
    "        X_trn, X_val = X.iloc[idx_trn, :], X.iloc[idx_val, :]\n",
    "        y_trn, y_val = y.iloc[idx_trn], y.iloc[idx_val]\n",
    "        X_trn = pd.DataFrame(X_trn, columns=X.columns)\n",
    "        X_val = pd.DataFrame(X_val, columns=X.columns)\n",
    "\n",
    "        # train\n",
    "        y_trn_pred, y_val_pred, _y_test_pred = run_fold_lgbm(X_trn, y_trn, X_val, y_val, \n",
    "                                                             X_te, model_config, show_log=show_log)\n",
    "\n",
    "        # result\n",
    "        oof[idx_val] = y_val_pred\n",
    "        y_test_pred += _y_test_pred / N_SPLITS\n",
    "\n",
    "\n",
    "    y_test_pred[y_test_pred <= 1] = 1\n",
    "    return oof, y_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(SAVE_DIR):\n",
    "    os.makedirs(SAVE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(PATH_TRAIN)\n",
    "test = pd.read_csv(PATH_TEST)\n",
    "ss = pd.read_csv(PATH_SAMPLE_SUBMITTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_preprocessing(train, feat_string)\n",
    "test = df_preprocessing(test, feat_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Platform',\n",
       " 'Year_of_Release',\n",
       " 'Genre',\n",
       " 'Publisher',\n",
       " 'Critic_Score',\n",
       " 'Critic_Count',\n",
       " 'User_Score',\n",
       " 'User_Count',\n",
       " 'Developer',\n",
       " 'Rating']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = test.columns != 'Name'\n",
    "use_col = test.columns[mask].tolist()\n",
    "use_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[use_col].copy()\n",
    "y = train[['Global_Sales']].copy()\n",
    "X_te = test[use_col].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = KFold(n_splits=N_SPLITS, shuffle=True, random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    'n_estimators': 800,\n",
    "    'max_depth': 55,\n",
    "    'subsample': 0.5,\n",
    "    'colsample_bytree': 0.9, \n",
    "#     'learning_rate': 0.006437110612661787,\n",
    "    'learning_rate': 0.01,\n",
    "    'reg_alpha': 5.0,\n",
    "    'reg_lambda': 5.0,\n",
    "    'min_child_samples': 57\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_col: ['Platform', 'Year_of_Release', 'Genre', 'Publisher', 'Critic_Score', 'Critic_Count', 'User_Score', 'User_Count', 'Developer', 'Rating']\n",
      "\n",
      "::Fold 1/5 start at Sat Dec  5 10:40:27 2020::\n",
      "True\n",
      "score train: 1.05393\n",
      "score valid: 1.11073\n",
      "\n",
      "::Fold 2/5 start at Sat Dec  5 10:40:28 2020::\n",
      "True\n",
      "score train: 1.03543\n",
      "score valid: 1.08560\n",
      "\n",
      "::Fold 3/5 start at Sat Dec  5 10:40:29 2020::\n",
      "True\n",
      "score train: 0.99053\n",
      "score valid: 1.12504\n",
      "\n",
      "::Fold 4/5 start at Sat Dec  5 10:40:30 2020::\n",
      "True\n",
      "score train: 1.01715\n",
      "score valid: 1.10029\n",
      "\n",
      "::Fold 5/5 start at Sat Dec  5 10:40:31 2020::\n",
      "True\n",
      "score train: 1.01704\n",
      "score valid: 1.13990\n",
      "\n",
      "oof score: 1.11247\n",
      "CPU times: user 53.4 s, sys: 272 ms, total: 53.6 s\n",
      "Wall time: 4.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "oof, y_test_pred = run(X, y, X_te, splitter, use_col, model_config, show_log=SHOW_LOG)\n",
    "print(f'oof score: {metric(y, oof):.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save: ../data/output_nb/nb010/submission.csv\n"
     ]
    }
   ],
   "source": [
    "ss['Global_Sales'] = y_test_pred\n",
    "save_path = f'{SAVE_DIR}submission.csv'\n",
    "ss.to_csv(save_path, index=False)\n",
    "\n",
    "print(f'save: {save_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
